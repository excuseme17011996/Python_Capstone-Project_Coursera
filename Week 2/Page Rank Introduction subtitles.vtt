WEBVTT

1
00:00:08.404 --> 00:00:11.058
So now we're going to talk about
our page rank application,

2
00:00:11.058 --> 00:00:14.250
and you can see the .zip file,
download the sample code for this, and so

3
00:00:14.250 --> 00:00:17.270
you can download that and
get this running on your computer.

4
00:00:17.270 --> 00:00:21.249
So what we're going to basically do is
write a Python program that crawls a web

5
00:00:21.249 --> 00:00:25.545
page, then we're going to run a simple
version of Google's Page Rank algorithm,

6
00:00:25.545 --> 00:00:29.779
which attempts to determine which pages
are better than other pages by counting

7
00:00:29.779 --> 00:00:31.502
the inbound links to that page.

8
00:00:31.502 --> 00:00:34.705
But then you only want to count
the links from good pages, so

9
00:00:34.705 --> 00:00:38.808
you have to count the links to those
pages, and the links to those pages, and

10
00:00:38.808 --> 00:00:42.142
eventually you kind of go sort
of infinitely until the value of

11
00:00:42.142 --> 00:00:46.350
each link stabilizes, and then you
kind of have the value of the page.

12
00:00:46.350 --> 00:00:49.090
And then we're going to visualize
the resulting network using

13
00:00:49.090 --> 00:00:53.080
a visualization software called D3,
just in a browser.

14
00:00:53.080 --> 00:00:54.560
And so this is the picture of it.

15
00:00:54.560 --> 00:00:57.884
We are going to kind of
come back to this picture.

16
00:00:57.884 --> 00:01:01.943
First, we are going to have,
the most important part of this process is

17
00:01:01.943 --> 00:01:05.799
the spidering process, which is
going to read pages from the Web, and

18
00:01:05.799 --> 00:01:09.115
going to keep track of the links
that have been followed and

19
00:01:09.115 --> 00:01:12.140
then the links that
are about to be followed.

20
00:01:12.140 --> 00:01:15.710
So spider's probably our most
sophisticated application,

21
00:01:15.710 --> 00:01:16.980
because it's restartable.

22
00:01:16.980 --> 00:01:18.490
That's the key to spider.

23
00:01:18.490 --> 00:01:19.140
We have the ability.

24
00:01:19.140 --> 00:01:21.380
We're going to put all
this stuff in a database.

25
00:01:21.380 --> 00:01:23.420
And we'll be able to dump that database.

26
00:01:23.420 --> 00:01:25.100
We'll be able to run
the ranking algorithm,

27
00:01:25.100 --> 00:01:26.800
which actually just changes the database.

28
00:01:26.800 --> 00:01:29.263
The ranking algorithm is a loop and

29
00:01:29.263 --> 00:01:34.090
it kind of keeps track of how accurate it
is and how much it's changing the network.

30
00:01:34.090 --> 00:01:37.210
If we want to restart things,
we can run this spreset.

31
00:01:37.210 --> 00:01:39.840
And then if we want to
visualize it we use spjson

32
00:01:39.840 --> 00:01:42.800
to dump the data into this force.js file.

33
00:01:42.800 --> 00:01:46.573
And then we visualize it
using force.html and d3.js.

34
00:01:46.573 --> 00:01:49.110
You won't really have to
write any of this code.

35
00:01:49.110 --> 00:01:53.023
The code we write will stop here,
it's just these Python programs.

36
00:01:53.023 --> 00:01:54.220
And that's what we're going to do.

37
00:01:55.240 --> 00:01:57.210
Let's take a look at the data model.

38
00:01:57.210 --> 00:02:00.540
The data model for
this application is pretty simple.

39
00:02:02.120 --> 00:02:06.040
It basically uses two pages, two tables.

40
00:02:06.040 --> 00:02:08.145
One table is the actual pages.

41
00:02:08.145 --> 00:02:12.885
And in here, during the spidering phase
we're going to to use this url, html, and

42
00:02:12.885 --> 00:02:16.625
error to indicate what we're
going to be retrieving.

43
00:02:16.625 --> 00:02:17.385
What happens is,

44
00:02:17.385 --> 00:02:21.315
we'll start at one URL, we'll retrieve it,
and we'll put the HTML for that.

45
00:02:21.315 --> 00:02:24.175
But then we'll use Beautiful Soup
to tear out all the links, and

46
00:02:24.175 --> 00:02:27.050
then we'll add new records to this, okay?

47
00:02:27.050 --> 00:02:30.860
And then what we have to do is,
this Pages is going to grow very rapidly.

48
00:02:30.860 --> 00:02:35.650
This Pages database is going to grow very
rapidly, mostly with unretrieved pages.

49
00:02:35.650 --> 00:02:38.500
So we'll get a page and
it'll have 20 links on it.

50
00:02:38.500 --> 00:02:41.288
So then we'll have one link that's been
retrieved and 20 that haven't retrieved.

51
00:02:41.288 --> 00:02:45.360
Then we'll go to the next link, and
that page will have 20 links on it.

52
00:02:45.360 --> 00:02:48.080
So we'll have two pages retrieved and
40 links.

53
00:02:48.080 --> 00:02:50.640
So the unretrieved grows really rapidly.

54
00:02:50.640 --> 00:02:53.890
And we're going to use this as,
we're going to basically say,

55
00:02:53.890 --> 00:02:58.658
when html is empty, that means it's
unretrieved, and then, well, or error.

56
00:02:58.658 --> 00:03:01.574
So if we hit pages that have errors,
then we're going to ignore them,

57
00:03:01.574 --> 00:03:03.477
and we'll mark down that
we had an an error.

58
00:03:03.477 --> 00:03:08.750
So if html is empty and error is empty,
then we're going to retrieve the page.

59
00:03:08.750 --> 00:03:11.260
And so then there's a loop
of code that goes up and

60
00:03:11.260 --> 00:03:14.130
says, do we have any unretrieved pages?

61
00:03:14.130 --> 00:03:19.096
Randomly grab an unretrieved page,
retrieve it, parse all the links, and

62
00:03:19.096 --> 00:03:21.434
then add to the end of the database.

63
00:03:21.434 --> 00:03:24.540
Now, as we do this,
we're going to have to capture the links.

64
00:03:24.540 --> 00:03:28.480
And so we're going to use a, it's a
many-to-many relation, but the reality is,

65
00:03:28.480 --> 00:03:31.800
it's a many-to-many relation
back onto the Pages table.

66
00:03:31.800 --> 00:03:37.150
So there's a link, so if we have a page
that has a link to another page, this

67
00:03:37.150 --> 00:03:41.560
is the from page and this is the to page,
and so we have a from_id and a to_id.

68
00:03:41.560 --> 00:03:45.090
So this is an arrow, the from_id 
to to_id.

69
00:03:45.090 --> 00:03:47.830
So the from_id links to a page.

70
00:03:49.940 --> 00:03:53.524
And so this might be 41,
and this might be 17, so

71
00:03:53.524 --> 00:03:56.304
that means page 41 links to page 17.

72
00:03:56.304 --> 00:04:01.200
And effectively, the primary key is
the page ID that we're going to use.

73
00:04:01.200 --> 00:04:06.620
Now, the way to think about this is
that it's really a, both ends of

74
00:04:06.620 --> 00:04:11.215
the foreign key, the from_id and
point to id, point back into the Pages.

75
00:04:11.215 --> 00:04:13.290
So for me, and mentally,
the way I draw is,

76
00:04:13.290 --> 00:04:17.000
I kind of pretend there's a second
copy of Pages over here, and so

77
00:04:17.000 --> 00:04:19.964
there's this many-to-one relationship,
many-to-one, you know,

78
00:04:19.964 --> 00:04:24.520
many-to-one, which is really just
a many-to-many between Pages and itself.

79
00:04:24.520 --> 00:04:27.355
So we'll have uniqueness constraints here,

80
00:04:27.355 --> 00:04:30.930
and we'll only allow one combination
of a from page and a to page.

81
00:04:30.930 --> 00:04:34.880
But the key is, the links are directional,
there's a from and a to.

82
00:04:34.880 --> 00:04:36.580
It's not symmetric.

83
00:04:36.580 --> 00:04:41.022
Some links are symmetric, in this case
with Pages, because we're going to take

84
00:04:41.022 --> 00:04:43.729
the goodness of the page rank and
flow them out.

85
00:04:43.729 --> 00:04:46.131
And then, so that's the retrieval process.

86
00:04:46.131 --> 00:04:49.520
We're going to focus on the url,
html, and error.

87
00:04:49.520 --> 00:04:54.851
And then the page rank, once we sort
of have all these links modeled,

88
00:04:54.851 --> 00:04:58.800
then we will have little page rank values,
two page rank values in each page,

89
00:04:58.800 --> 00:05:00.900
and there's the old one and the new one.

90
00:05:00.900 --> 00:05:06.510
So what it does is it takes the old page
rank value and kind of transfers it out,

91
00:05:06.510 --> 00:05:11.690
and then computes the new one, and
then it rotates all the news to the olds,

92
00:05:11.690 --> 00:05:13.810
and then it does it again and
again and again.

93
00:05:13.810 --> 00:05:15.630
And we'll run this over and
over and over again.

94
00:05:15.630 --> 00:05:19.860
So, the page rank is really looking
at this bit of data right here.

95
00:05:19.860 --> 00:05:20.640
Okay?

96
00:05:20.640 --> 00:05:23.240
So let's go ahead and get started
taking a look at some of the code.

97
00:05:25.010 --> 00:05:28.490
Okay, so let me go ahead and run it.

98
00:05:31.170 --> 00:05:35.907
Now, I'm sitting here in a pagerank
folder that you would have somewhere on

99
00:05:35.907 --> 00:05:39.831
your hard drive, pagerank.zip,
the file spider.sqlite,

100
00:05:39.831 --> 00:05:43.697
you can delete that anytime you
want to start this thing over.

101
00:05:43.697 --> 00:05:45.114
python spider.py.

102
00:05:45.114 --> 00:05:50.622
Because we're first starting out and
it sees no, sees nothing in the database,

103
00:05:50.622 --> 00:05:54.986
it basically says,
where would you like me to start.

104
00:05:54.986 --> 00:05:58.500
Hit Enter, it will actually start at
the Python data that we've been using for

105
00:05:58.500 --> 00:05:59.490
the class.

106
00:05:59.490 --> 00:06:01.070
So I'm just going to hit Enter.

107
00:06:01.070 --> 00:06:04.678
So it says, okay, I'll start at
the top of python-data.dr-chuck.net.

108
00:06:04.678 --> 00:06:08.567
So if you go to this page, you will
see that it's mostly like five links and

109
00:06:08.567 --> 00:06:10.370
then it goes into our names.

110
00:06:10.370 --> 00:06:11.202
Let me show you that page.

111
00:06:11.202 --> 00:06:12.769
Let me just remind you
what that page looks like.

112
00:06:16.171 --> 00:06:18.136
Python-data.dr-chuck.net.

113
00:06:19.620 --> 00:06:21.530
So it's a series of links.

114
00:06:21.530 --> 00:06:25.970
This thing will not leave the website,
otherwise our page rank would never work.

115
00:06:25.970 --> 00:06:30.130
And so what it's really going to do is
going to go into this set of friend lists

116
00:06:30.130 --> 00:06:35.320
and then, once it gets that data, then
it's going to have another set of links.

117
00:06:35.320 --> 00:06:36.420
File this link.

118
00:06:36.420 --> 00:06:37.506
So that's what it's going to do,

119
00:06:37.506 --> 00:06:39.840
it's going to start, each one of these
pages is going to have a bunch of links.

120
00:06:39.840 --> 00:06:43.890
Now, the way this works is, you've been
playing with this in other assignments,

121
00:06:43.890 --> 00:06:47.530
but it turns out that some people
have more friends than others.

122
00:06:47.530 --> 00:06:50.732
And that's actually what we're going to
figure out in this particular page rank,

123
00:06:50.732 --> 00:06:52.530
is who has the most friends.

124
00:06:52.530 --> 00:06:53.380
Okay?

125
00:06:53.380 --> 00:06:58.360
And so, the key thing, if we look at
this whole process, if we look at

126
00:06:58.360 --> 00:07:01.392
the process that we're working on right
now, we're working on this process.

127
00:07:01.392 --> 00:07:06.380
And the key to any process like this, it's
slow, it uses network bandwidth, and

128
00:07:06.380 --> 00:07:09.960
you want to make sure you're committing
to the database every time through.

129
00:07:09.960 --> 00:07:12.600
You retrieve a page,
stick it in the database, and commit.

130
00:07:12.600 --> 00:07:14.400
Retrieve a page, stick it in the database.

131
00:07:14.400 --> 00:07:18.731
We've had some situations where that has
gotten kind of slow at times because we're

132
00:07:18.731 --> 00:07:23.007
committing so often, but the retrieval is
slow, so we don't want to lose any data.

133
00:07:23.007 --> 00:07:25.590
That's why you do the commit in databases.

134
00:07:25.590 --> 00:07:27.980
But we want to be able to stop this and
start this.

135
00:07:27.980 --> 00:07:30.350
We'll either tell it to do ten and
then start over and

136
00:07:30.350 --> 00:07:34.960
it'll pick it where it left off,
or we will tell it to 100 and

137
00:07:34.960 --> 00:07:38.690
then just hit Ctrl+C or
Ctrl+Z to blow it up.

138
00:07:38.690 --> 00:07:40.805
Let me start the SQLite browser too, so

139
00:07:40.805 --> 00:07:43.377
you can kind of watch what's
going to happen here.

140
00:07:46.815 --> 00:07:49.980
So, I'm only going to retrieve one page.

141
00:07:52.030 --> 00:07:55.890
Okay, and so that retrieved one page, and
I'm just going to hit Enter to get out of

142
00:07:55.890 --> 00:07:58.030
that, and
now I'm going to take a look at that file.

143
00:07:59.470 --> 00:08:03.290
That file is spider.sqlite, okay?

144
00:08:03.290 --> 00:08:06.918
And so now I'm going to open that in my
SQLite browser so we can take a look at

145
00:08:06.918 --> 00:08:10.742
what's going on and see the kind of
the structure of what's going on here.

146
00:08:10.742 --> 00:08:11.474
Open Database.

147
00:08:14.875 --> 00:08:15.584
Desktop.

148
00:08:17.280 --> 00:08:18.161
pythonlearn.

149
00:08:21.842 --> 00:08:22.561
code.

150
00:08:28.134 --> 00:08:32.434
pagerank and then spider.sqlite.

151
00:08:33.500 --> 00:08:34.000
Open.

152
00:08:35.700 --> 00:08:37.230
So we have Links, Pages, and Webs.

153
00:08:37.230 --> 00:08:41.350
Webs is just a little scratch space
to keep track of what's going on.

154
00:08:43.090 --> 00:08:45.490
If I say Browse Data, and
I take a look at Links.

155
00:08:46.560 --> 00:08:48.460
Start with Webs.

156
00:08:48.460 --> 00:08:52.380
This is just keeping track of which page
we started with so it can start back up.

157
00:08:52.380 --> 00:08:55.243
But once it gets started,
it actually is more interested in Pages.

158
00:08:58.562 --> 00:09:01.540
So it's retrieved one page, okay?

159
00:09:01.540 --> 00:09:05.240
And so it retrieved the page,
it actually has the HTML for

160
00:09:05.240 --> 00:09:08.650
that page, and it doesn't have an error,
which is okay.

161
00:09:08.650 --> 00:09:10.588
It's either going to get you html or
error.

162
00:09:10.588 --> 00:09:11.433
And then old rank and

163
00:09:11.433 --> 00:09:15.570
new rank are what we're going to use when
we're doing the page rank calculation.

164
00:09:15.570 --> 00:09:18.940
Okay?
And so that's what we look like right now.

165
00:09:18.940 --> 00:09:24.140
And if I start the program again,
I run spider again,

166
00:09:24.140 --> 00:09:29.160
it's going to actually,
what it does is it looks for any URL with

167
00:09:29.160 --> 00:09:32.890
the html null and error null and says, oh
that's something we haven't touched yet,

168
00:09:32.890 --> 00:09:37.760
and then it goes and grabs that page, and
then it looks for links in that page.

169
00:09:37.760 --> 00:09:38.600
Oh. Hang on.

170
00:09:38.600 --> 00:09:40.740
Gotta show you all this stuff
before we get too complex,

171
00:09:40.740 --> 00:09:42.980
because it's going to get big really fast.

172
00:09:42.980 --> 00:09:47.480
If we switch and we look at Links,
so we have these seven pages.

173
00:09:47.480 --> 00:09:50.630
We've only retrieved one of them,
but we have lots of links.

174
00:09:50.630 --> 00:09:52.410
1 links to 2, 1 links to 3.

175
00:09:52.410 --> 00:09:54.710
This is the from_id,
this is where it's looping back.

176
00:09:54.710 --> 00:09:58.968
1 links to 2, 1 links to 3, 1 links to 4,
1 links to 5, 1 links to 6,

177
00:09:58.968 --> 00:10:00.108
1 links to 7.

178
00:10:00.108 --> 00:10:04.970
So we retrieved page one and
it caused six more pages, and

179
00:10:04.970 --> 00:10:07.090
we recorded the fact
that they are the links.

180
00:10:07.090 --> 00:10:07.590
Okay?

181
00:10:09.060 --> 00:10:11.530
And so now we're ready to
retrieve the second page.

182
00:10:11.530 --> 00:10:16.550
And I'll just retrieve one more page, and
we're going to retrieve one more page.

183
00:10:16.550 --> 00:10:20.569
So it grabbed, well, okay,
that was, it found an XML page.

184
00:10:20.569 --> 00:10:21.891
So let's go get one more page.

185
00:10:21.891 --> 00:10:26.871
Well, okay, I can show you real
quick what happens hopefully there.

186
00:10:26.871 --> 00:10:27.628
Pages.

187
00:10:31.449 --> 00:10:37.104
Let's get out and
then hit refresh here, hit refresh.

188
00:10:45.105 --> 00:10:49.680
I think it just deleted that page.

189
00:10:49.680 --> 00:10:55.100
So let's grab another page,
one more, okay.

190
00:10:55.100 --> 00:11:02.490
So that got seven more links, so
now if I refresh the Pages, okay.

191
00:11:02.490 --> 00:11:06.360
So it retrieved this page, and you see it
found a whole bunch more links, right?

192
00:11:06.360 --> 00:11:07.600
There's all these links.

193
00:11:07.600 --> 00:11:10.392
So it's only retrieved two pages, but

194
00:11:10.392 --> 00:11:14.548
that page that it retrieved
had a bunch of outbound links.

195
00:11:14.548 --> 00:11:18.623
And so if we look at the links,
there are tons of them.

196
00:11:18.623 --> 00:11:23.544
Page 1 links to 6, 1 links to 7,
7 links to 8, 9, 10, 11.

197
00:11:23.544 --> 00:11:24.925
All these unretrieved pages.

198
00:11:24.925 --> 00:11:27.409
Okay?

199
00:11:27.409 --> 00:11:29.969
So let's take a quick look
at some of this code so

200
00:11:29.969 --> 00:11:32.670
you can see how this turns
out to work inside code.

201
00:11:33.750 --> 00:11:37.220
Up next, we're going to look at some code
to see how this all works in the code.